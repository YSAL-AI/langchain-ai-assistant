import streamlit as st
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.llms import OpenAI
from langchain.document_loaders import PyPDFLoader
import os

st.set_page_config(page_title="LangChain AI Assistant", page_icon="ðŸ¤–")

st.title("LangChain AI Assistant with Memory")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

uploaded_file = st.file_uploader("Upload a PDF file", type=["pdf"])

if uploaded_file:
    with open("uploaded.pdf", "wb") as f:
        f.write(uploaded_file.getbuffer())
    loader = PyPDFLoader("uploaded.pdf")
    documents = loader.load()
    st.success(f"Loaded {len(documents)} pages from uploaded PDF.")

    # Here you would create embeddings and vector store
    # (For simplicity, omitted - will explain later)

query = st.text_input("Ask me anything:")

if query:
    # Dummy response for now, you can replace with actual chain calls
    response = f"You asked: {query}"
    st.session_state.chat_history.append((query, response))

for i, (q, a) in enumerate(st.session_state.chat_history):
    st.markdown(f"**You:** {q}")
    st.markdown(f"**AI:** {a}")
